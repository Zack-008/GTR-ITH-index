{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pingouin as pg\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from mirp import extract_features\n",
    "from mirp.settings.perturbation_parameters import ImagePerturbationSettingsClass\n",
    "from sklearn.mixture import GaussianMixture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perturbation  (GTR & ITH Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path='/path/to/your/data'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_label(row):\n",
    "    if row['image_noise_level'] == 0 and pd.isna(row['image_noise_iteration_id']) and row['image_rotation_angle'] == 0 and row['image_translation_x'] == 0 and row['image_translation_y'] == 0 and row['image_translation_z'] == 0:\n",
    "        return 'r1'\n",
    "    elif row['image_noise_iteration_id'] == 0 and row['image_rotation_angle'] == 0.5 and row['image_translation_x'] == 0.5 and row['image_translation_y'] == 0.5 and row['image_translation_z'] == 0.5:\n",
    "        return 'r2'\n",
    "    elif row['image_noise_iteration_id'] == 1 and row['image_rotation_angle'] == 0.5 and row['image_translation_x'] == 0.5 and row['image_translation_y'] == 0.5 and row['image_translation_z'] == 0.5:\n",
    "        return 'r3'\n",
    "    else:\n",
    "        return 'other' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataset(list_volume, list_mask=None):\n",
    "\n",
    "    if list_mask is None:  \n",
    "        lists_to_check = [list_volume]\n",
    "    else: \n",
    "        if len(list_volume) != len(list_mask):\n",
    "            raise ValueError('There exists a mismatch between two datasets.')\n",
    "        lists_to_check = [list_volume, list_mask]\n",
    "    \n",
    "    errors_df = pd.DataFrame(columns=['File_Path', 'Type'])\n",
    "    \n",
    "    for list_index, file_list in enumerate(lists_to_check):\n",
    "        list_type = 'Volume' if list_index == 0 else 'Mask'\n",
    "        for file_path in file_list:\n",
    "            if not os.path.exists(file_path):\n",
    "                \n",
    "                temp_df = pd.DataFrame({'File_Path': [file_path], 'Type': [list_type]})\n",
    "                errors_df = pd.concat([errors_df, temp_df], ignore_index=True)\n",
    "    \n",
    "    if not errors_df.empty:\n",
    "        errors_df.to_excel('/path/to/errors_file_paths.xlsx', index=False)\n",
    "        print(\"Invalid file paths were found and have been saved to “errors_file_paths.xlsx”.\")\n",
    "    else:\n",
    "        print(\"All file paths exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_filtered_files = glob.glob(os.path.join(source_path,'perturbation','Label_SV', '*'))\n",
    "\n",
    "image_files = []\n",
    "\n",
    "for file_path in mask_filtered_files:\n",
    "    new_filename =os.path.join(source_path,'ICC_calculate','Image_SV','_'.join(os.path.basename(file_path).split('_')[3:-2])+ \"_image_\"+os.path.basename(file_path).split('_')[-1])\n",
    "    image_files.append(new_filename)\n",
    "\n",
    "check_dataset(image_files, mask_filtered_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(image_batch, mask_batch, settings, output_file, append=False):\n",
    "    feature_data = extract_features(\n",
    "        image=image_batch,\n",
    "        mask=mask_batch,\n",
    "        base_discretisation_method=\"fixed_bin_number\",\n",
    "        image_modality=\"CT\",\n",
    "        base_discretisation_bin_width=16.0,\n",
    "        settings=settings\n",
    "    )\n",
    "    combined_df = pd.concat(feature_data, ignore_index=True)\n",
    "    \n",
    "    write_mode = 'a' if append else 'w'\n",
    "    header = not append \n",
    "    combined_df.to_csv(output_file, mode=write_mode, header=header, index=False)\n",
    "\n",
    "\n",
    "settings = os.path.join(source_path,'perturbation','perturbation_test_config_settings.xml')\n",
    "output_file = os.path.join(source_path,'perturbation','Perturbation_featurelevel_output.csv')\n",
    "\n",
    "batch_size = 10  \n",
    "num_batches = len(image_files) // batch_size + (1 if len(image_files) % batch_size > 0 else 0)\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "    image_batch = image_files[start_idx:end_idx]\n",
    "    mask_batch = mask_filtered_files[start_idx:end_idx]\n",
    "    process_batch(image_batch, mask_batch, settings, output_file, append=i > 0)\n",
    "print(\"Finish for all batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Perturbation_featurelevel = pd.read_csv(os.path.join(source_path,'ICC_calculate','Perturbation_featurelevel_output.csv'))\n",
    "\n",
    "Perturbation_featurelevel['reader'] = Perturbation_featurelevel.apply(assign_label, axis=1)\n",
    "Perturbation_featurelevel_cleaned = Perturbation_featurelevel.dropna(subset=['cm_joint_var_d1_3d_v_mrg_fbs_w16.0', 'ngl_ldlge_d1_a0.0_3d_fbs_w16.0'])\n",
    "\n",
    "prefixes_to_keep = ['stat_', 'ivh_', 'morph_', 'ih_', 'cm_', 'rlm_','szm_', 'dzm_','ngt_','ngl_']\n",
    "columns_to_keep = ['sample_name','reader'] + [col for col in Perturbation_featurelevel.columns if any(col.startswith(prefix) for prefix in prefixes_to_keep)]\n",
    "\n",
    "ICC_Pert = Perturbation_featurelevel[columns_to_keep].copy() \n",
    "\n",
    "ICC_Pert['sample_name'] = ICC_Pert['sample_name'].astype(str)\n",
    "ICC_Pert['reader'] = ICC_Pert['reader'].astype(str)\n",
    "\n",
    "icc_Pert_results =  []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ICC_Pert.columns[2:]:\n",
    "    icc_data = ICC_Pert[['sample_name', 'reader', column]].copy()\n",
    "    icc_data = icc_data.pivot(index='sample_name', columns='reader', values=column).reset_index()\n",
    "    icc_data.columns.name = None  \n",
    "    \n",
    "    icc_data_melted = icc_data.melt(id_vars='sample_name', value_vars=['r1', 'r2', 'r3'], var_name='reader', value_name=column)\n",
    "    icc = pg.intraclass_corr(data=icc_data_melted, targets='sample_name', raters='reader', ratings=column, nan_policy='omit').round(3)\n",
    "    icc['feature'] = column  \n",
    "    icc_Pert_results.append(icc)\n",
    "\n",
    "per_results_df = pd.concat(icc_Pert_results, ignore_index=True)\n",
    "per_results_df.to_csv(\"/path/to/perturbation_result_featurelevel.csv\")\n",
    "print(\"ICC calculation complete!\")\n",
    "\n",
    "per_filtered_df = per_results_df[(per_results_df['Type'] == 'ICC3k') & (per_results_df['ICC'] <= 0.75)] \n",
    "per_features_remove = per_filtered_df['feature'].unique()\n",
    "len(per_features_remove)\n",
    "\n",
    "# These features need to align with corresponding pyradiomics features.\n",
    "mapping = pd.read_excel(\"/path/to/feature_correspond.xlsx\")\n",
    "mapping = mapping.dropna()\n",
    "mapping_dict = pd.Series(mapping.new_name.values, index=mapping.sample_name).to_dict()\n",
    "replaced_features = []\n",
    "for feature in per_features_remove:\n",
    "\n",
    "    if pd.isna(feature):\n",
    "        replaced_features.append(feature)\n",
    "        continue\n",
    "    \n",
    "    replaced = False\n",
    "    for partial_name in mapping_dict.keys():\n",
    "        if partial_name in feature:\n",
    "            replaced_features.append(mapping_dict[partial_name])\n",
    "            replaced = True\n",
    "            break\n",
    "    if not replaced:\n",
    "        replaced_features.append(feature)\n",
    "\n",
    "replaced_features = np.array(replaced_features)\n",
    "print(replaced_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_removed_features_SV = np.unique(replaced_features)  \n",
    "len(combined_removed_features_SV) \n",
    "\n",
    "final_combined_removed_features_SV = pd.DataFrame(combined_removed_features_SV, columns=['removed_Feature'])\n",
    "final_combined_removed_features_SV.to_excel('/path/to/Removed_features_ITH.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Dimensionality Reduction--Patient Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "return the K value and covariance_type from the best GMM model according to the BIC\n",
    "'''\n",
    "def get_K(feature, K_num):    \n",
    "    lowest_bic = np.infty   \n",
    "    bic = []                 \n",
    "    n_components_range = range(1, K_num+1)     \n",
    "    cv_types = [\"full\"]  \n",
    "\n",
    "    for cv_type in cv_types:      \n",
    "        for n_components in n_components_range:\n",
    "            gmm = GaussianMixture(\n",
    "                n_components=n_components, covariance_type=cv_type,random_state=0\n",
    "            )\n",
    "            gmm.fit(feature)\n",
    "            bic.append(gmm.bic(feature))    \n",
    "            if bic[-1] < lowest_bic:\n",
    "                lowest_bic = bic[-1]\n",
    "                best_gmm = gmm\n",
    "\n",
    "    bic = np.array(bic)         \n",
    "    return best_gmm.n_components, best_gmm.covariance_type       \n",
    "\n",
    "'''\n",
    "For each feature, assign a K value based on GMM(BIC)\n",
    "Assume each patient has N features, return a list with shape (1,N) which include K for each feature\n",
    "'''\n",
    "def assign_K_value2features(Patient_features, K_num):   \n",
    "    x, y = Patient_features.shape        \n",
    "    K_list = []                  \n",
    "    cov_type_list = []\n",
    "    for feature_idx in range(0, y):       \n",
    "        X_train = np.array(Patient_features.iloc[:, feature_idx]).reshape(-1,1)\n",
    "        k, cov_type = get_K(X_train, K_num)\n",
    "        K_list.append(k)\n",
    "        cov_type_list.append(cov_type)\n",
    "\n",
    "    return K_list, cov_type_list   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_patient_features(Patient_features, K_list, cov_type_list):\n",
    "    cluster_labels_dict = {} \n",
    "    for feature_idx, (k, cov_type) in enumerate(zip(K_list, cov_type_list)):\n",
    "        feature_name = Patient_features.columns[feature_idx]  \n",
    "        X_train = np.array(Patient_features.iloc[:, feature_idx]).reshape(-1, 1)\n",
    "        gmm = GaussianMixture(n_components=k, covariance_type=cov_type, random_state=0)  \n",
    "        gmm.fit(X_train)  \n",
    "        labels = gmm.predict(X_train) \n",
    "        cluster_labels_dict[feature_name + '_Cluster'] = labels \n",
    "\n",
    "    cluster_labels_df = pd.DataFrame(cluster_labels_dict)\n",
    "    \n",
    "    return cluster_labels_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features(df_stand, id_columns):\n",
    "    if isinstance(id_columns, str): \n",
    "        id_columns = [id_columns]\n",
    "\n",
    "    df_stand.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    features_to_scale = df_stand.drop(columns=id_columns)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features_to_scale)\n",
    "    \n",
    "    scaled_features_df = pd.DataFrame(scaled_features, columns=features_to_scale.columns)\n",
    "    \n",
    "    id_columns_df = df_stand[id_columns]\n",
    "    \n",
    "    result_df = pd.concat([id_columns_df, scaled_features_df], axis=1)\n",
    "    \n",
    "    return result_df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dir = os.path.join(source_path,'analysis')\n",
    "cluster_dir = os.path.join(source_path,'analysis','cluster_test')\n",
    "\n",
    "df_clinical = pd.read_excel(\"/path/to/clinical_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature-Level的聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intratumor Feature-Level的聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pyradiomics_SV = pd.read_excel(os.path.join(cluster_dir,'Feature_level','Training_featurelevel_pyradiomics.xlsx'))\n",
    "\n",
    "combined_removed_features_featurelevel =  final_combined_removed_features_SV\n",
    "prefixes_features_featurelevel_remove = combined_removed_features_featurelevel[\"removed_Feature\"].tolist() \n",
    "training_pyradiomics_cluster_SV= training_pyradiomics_SV.drop(columns=prefixes_features_featurelevel_remove, errors='ignore') \n",
    "\n",
    "columns_to_drop = training_pyradiomics_cluster_SV.filter(like='original_shape').columns  \n",
    "training_pyradiomics_cluster_SV=training_pyradiomics_cluster_SV.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "df_numeric = training_pyradiomics_cluster_SV.drop([\"ID\",'CaseID'], axis=1).apply(pd.to_numeric, errors='coerce').reset_index(drop=True)\n",
    "rows_with_non_numeric = df_numeric.isnull().any(axis=1)\n",
    "training_inval_set = training_pyradiomics_cluster_SV[~rows_with_non_numeric]\n",
    "\n",
    "\n",
    "train_set, training_scaler = standardize_features(training_inval_set, ['ID','CaseID'])\n",
    "train_set= train_set.drop(columns=\"CaseID\", errors='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Patients_K = pd.DataFrame()\n",
    "cluster_labels_all = pd.DataFrame()\n",
    "ptid = []\n",
    "Patients_covtype = []\n",
    "ptid_K0 = []\n",
    "\n",
    "K_num = 5\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "\n",
    "for ptid, group in train_set.groupby('CaseID'):       \n",
    "    \n",
    "    try:\n",
    "        analysis_data = group.drop(['CaseID'], axis=1)\n",
    "        K, cov_type_list = assign_K_value2features(analysis_data.drop(['ID'], axis=1), K_num)  \n",
    "        if sum(K) != 0:\n",
    "            cluster_labels_df = cluster_patient_features(analysis_data.drop(['ID'], axis=1), K, cov_type_list)\n",
    "\n",
    "            cluster_labels_df['ID'] = analysis_data['ID'].values  \n",
    "            cluster_labels_df['CaseID'] = ptid\n",
    "\n",
    "            cols = cluster_labels_df.columns.tolist()\n",
    "            cols = ['ID', 'CaseID'] + [col for col in cols if col not in ['ID', 'CaseID']]\n",
    "            cluster_labels_df = cluster_labels_df[cols]\n",
    "            cluster_labels_all = pd.concat([cluster_labels_all, cluster_labels_df], ignore_index=True, axis=0)\n",
    "\n",
    "            K_new = [ptid] + K  \n",
    "            K_new_df=pd.DataFrame([K_new], columns=['CaseID'] +  list(train_set.columns.drop(['ID','CaseID'])))\n",
    "            Patients_K = pd.concat([Patients_K, K_new_df], ignore_index=True, axis=0)\n",
    "            # print(f'{ptid} is done')\n",
    "        else:\n",
    "            ptid_K0.append(ptid)\n",
    "            print(f'something wrong: K is 0 for patient {ptid}')\n",
    "    except Exception as e:\n",
    "        print(f'Error: something wrong about K for patient {ptid}, Error: {e}')\n",
    "    finally:\n",
    "        Patients_covtype.append((ptid, cov_type_list))\n",
    "\n",
    "warnings.filterwarnings('always', category=UserWarning, module='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Patients_K_IDrep = pd.merge(training_pyradiomics_SV[['CaseID', 'ID_rep']].drop_duplicates(), Patients_K, on='CaseID', how='left')\n",
    "Patients_K_IDrep.to_excel(os.path.join(cluster_dir,\"training_featurelevel_Kvalue.xlsx\"), index=False)\n",
    "cluster_labels_all.to_excel(\"/path/to/training_featurelevel_cluster.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCC+Ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Pearson Correlation Coefficient (PCC) to remove highly correlated features\n",
    "corr_matrix = Patients_K.drop(['CaseID'], axis=1).corr()  \n",
    "\n",
    "high_corr_var=np.where(corr_matrix>0.75)  \n",
    "\n",
    "to_remove = set()  \n",
    "\n",
    "for var in zip(*high_corr_var):  \n",
    "    if var[0] != var[1] and var[0] not in to_remove: \n",
    "        to_remove.add(var[1]) \n",
    "\n",
    "df_filtered = Patients_K.drop(columns=Patients_K.columns[list(to_remove)]) \n",
    "pcc_features_remove = corr_matrix.columns[list(to_remove)]\n",
    "len(pcc_features_remove)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "降维分析--独立样本检验--ORR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = pd.merge(df_filtered, training_pyradiomics_SV[['CaseID']].drop_duplicates(), on='CaseID', how='left')\n",
    "df_Ttest = pd.merge(df_filtered, df_clinical[['CaseID', 'ORR_RECIST1.1']], on='CaseID', how='left')\n",
    "\n",
    "columns = list(df_Ttest.columns)\n",
    "columns.remove('ORR_RECIST1.1')\n",
    "columns.insert(1, 'ORR_RECIST1.1')\n",
    "df_Ttest = df_Ttest[columns] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Significant features: ['original_firstorder_Maximum', 'original_firstorder_Mean', 'original_firstorder_Minimum', 'original_firstorder_RootMeanSquared', 'original_firstorder_Variance', 'original_glcm_Autocorrelation', 'original_glcm_ClusterTendency', 'original_glcm_Contrast', 'original_glcm_DifferenceVariance', 'original_glcm_Idn', 'original_glcm_SumSquares', 'original_glrlm_HighGrayLevelRunEmphasis', 'original_glrlm_RunEntropy', 'original_glszm_GrayLevelNonUniformityNormalized', 'original_glszm_GrayLevelVariance', 'original_glszm_SmallAreaEmphasis', 'original_gldm_DependenceEntropy', 'original_gldm_GrayLevelVariance', 'original_ngtdm_Busyness', 'original_ngtdm_Complexity']\n",
      "                                              Feature       P-Value\n",
      "0                    original_firstorder_10Percentile  9.169812e-01\n",
      "1                          original_firstorder_Energy  6.183357e-02\n",
      "2                         original_firstorder_Entropy  1.324318e-01\n",
      "3              original_firstorder_InterquartileRange  7.582735e-01\n",
      "4                         original_firstorder_Maximum  5.589713e-06\n",
      "5           original_firstorder_MeanAbsoluteDeviation  2.086571e-01\n",
      "6                            original_firstorder_Mean  2.205551e-02\n",
      "7                          original_firstorder_Median  1.622458e-01\n",
      "8                         original_firstorder_Minimum  1.470875e-04\n",
      "9                 original_firstorder_RootMeanSquared  1.729808e-02\n",
      "10                    original_firstorder_TotalEnergy  6.183357e-02\n",
      "11                     original_firstorder_Uniformity  2.784139e-01\n",
      "12                       original_firstorder_Variance  1.902660e-02\n",
      "13                      original_glcm_Autocorrelation  6.760268e-04\n",
      "14                      original_glcm_ClusterTendency  2.076453e-02\n",
      "15                             original_glcm_Contrast  3.838899e-02\n",
      "16                          original_glcm_Correlation  5.729371e-02\n",
      "17                    original_glcm_DifferenceAverage  2.767900e-01\n",
      "18                   original_glcm_DifferenceVariance  3.242873e-04\n",
      "19                         original_glcm_JointEntropy  8.990485e-02\n",
      "20                                 original_glcm_Imc1  4.107196e-01\n",
      "21                                 original_glcm_Imc2  5.863732e-01\n",
      "22                                  original_glcm_Idm  3.533490e-01\n",
      "23                                   original_glcm_Id  2.151813e-01\n",
      "24                                  original_glcm_Idn  2.779533e-12\n",
      "25                           original_glcm_SumSquares  3.367597e-02\n",
      "26            original_glrlm_HighGrayLevelRunEmphasis  1.432480e-03\n",
      "27             original_glrlm_LowGrayLevelRunEmphasis  8.704427e-01\n",
      "28                          original_glrlm_RunEntropy  9.707514e-04\n",
      "29              original_glrlm_RunLengthNonUniformity  9.428801e-01\n",
      "30    original_glrlm_RunLengthNonUniformityNormalized  6.556730e-01\n",
      "31                       original_glrlm_RunPercentage  7.644083e-01\n",
      "32        original_glrlm_ShortRunLowGrayLevelEmphasis  5.169500e-01\n",
      "33              original_glszm_GrayLevelNonUniformity  6.923994e-01\n",
      "34    original_glszm_GrayLevelNonUniformityNormalized  1.629765e-03\n",
      "35                   original_glszm_GrayLevelVariance  9.836482e-04\n",
      "36                   original_glszm_LargeAreaEmphasis  8.388314e-02\n",
      "37       original_glszm_LargeAreaLowGrayLevelEmphasis  6.405522e-01\n",
      "38            original_glszm_LowGrayLevelZoneEmphasis  9.355637e-01\n",
      "39               original_glszm_SizeZoneNonUniformity  4.841787e-01\n",
      "40     original_glszm_SizeZoneNonUniformityNormalized  1.212455e-01\n",
      "41                   original_glszm_SmallAreaEmphasis  4.615049e-02\n",
      "42       original_glszm_SmallAreaLowGrayLevelEmphasis  9.162918e-01\n",
      "43                        original_glszm_ZoneVariance  1.761235e-01\n",
      "44                    original_gldm_DependenceEntropy  1.151581e-02\n",
      "45              original_gldm_DependenceNonUniformity  3.637662e-01\n",
      "46    original_gldm_DependenceNonUniformityNormalized  2.781095e-01\n",
      "47                   original_gldm_DependenceVariance  8.207965e-01\n",
      "48                    original_gldm_GrayLevelVariance  1.332810e-02\n",
      "49              original_gldm_LargeDependenceEmphasis  5.805238e-01\n",
      "50                 original_gldm_LowGrayLevelEmphasis  7.280413e-01\n",
      "51              original_gldm_SmallDependenceEmphasis  9.683899e-02\n",
      "52  original_gldm_SmallDependenceLowGrayLevelEmphasis  6.043365e-01\n",
      "53                            original_ngtdm_Busyness  2.294737e-02\n",
      "54                          original_ngtdm_Complexity  1.033474e-08\n",
      "55                            original_ngtdm_Contrast  8.947025e-01\n"
     ]
    }
   ],
   "source": [
    "group_0 = df_Ttest[df_Ttest['ORR_RECIST1.1'] == 0]  \n",
    "group_1 = df_Ttest[df_Ttest['ORR_RECIST1.1'] == 1] \n",
    "\n",
    "features = df_Ttest.columns[2:]\n",
    "\n",
    "significant_features = []\n",
    "p_values = []\n",
    "for feature in features:\n",
    "    stat, p_value = ttest_ind(group_0[feature], group_1[feature])\n",
    "    p_values.append(p_value)  \n",
    "    if p_value < 0.05: \n",
    "        significant_features.append(feature)\n",
    "\n",
    "print(len(significant_features), \"Significant features:\", significant_features)\n",
    "\n",
    "p_values_df = pd.DataFrame({'Feature': features, 'P-Value': p_values})\n",
    "print(p_values_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features_patientlevel = pd.DataFrame(significant_features, columns=['Feature'])\n",
    "final_features_patientlevel.to_excel(os.path.join(analysis_dir,'ICC_calculate','final_features_featurelevel_list.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "内部测试集的聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##内部测试集\n",
    "inval_set_pyradiomics_SV = pd.read_excel(os.path.join(cluster_dir,'Feature_level','Inval_featurelevel_pyradiomics.xlsx'))\n",
    "columns_to_keep = ['CaseID','ID'] + [col for col in inval_set_pyradiomics_SV.columns if col in significant_features]  # 生成要保留的列名列表\n",
    "\n",
    "inval_pyradiomics_SV = inval_set_pyradiomics_SV[columns_to_keep].copy()  # 保留指定的列\n",
    "\n",
    "## 【去掉有错误的行】使用applymap和pd.to_numeric尝试将所有值转换为数值，无法转换的设置为NaN\n",
    "df_numeric_inval = inval_pyradiomics_SV.drop([\"ID\",'CaseID'], axis=1).apply(pd.to_numeric, errors='coerce')\n",
    "rows_with_non_numeric = df_numeric_inval.isnull().any(axis=1)\n",
    "inval_pyradiomics_cluster_test_SV = inval_pyradiomics_SV[~rows_with_non_numeric].reset_index(drop=True)\n",
    "\n",
    "## 训练集的标准化\n",
    "df_temp=training_pyradiomics_SV[['ID']+[col for col in train_set.columns if col in significant_features]]\n",
    "df_numeric_inval = df_temp.drop([\"ID\"], axis=1).apply(pd.to_numeric, errors='coerce')\n",
    "rows_with_non_numeric = df_numeric_inval.isnull().any(axis=1)\n",
    "df_temp = df_temp[~rows_with_non_numeric]\n",
    "training_df_temp_scaler, training_scaler = standardize_features(df_temp,\"ID\")\n",
    "\n",
    "\n",
    "features_to_scale_validation_SV = inval_pyradiomics_cluster_test_SV.drop(['CaseID','ID'], axis=1)\n",
    "scaled_features_validation_SV  = training_scaler.transform(features_to_scale_validation_SV)\n",
    "\n",
    "scaled_features_validation_df = pd.DataFrame(scaled_features_validation_SV, columns=features_to_scale_validation_SV.columns)\n",
    "df_validation_standardized = pd.concat([inval_pyradiomics_cluster_test_SV[['CaseID', 'ID']], scaled_features_validation_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 开始聚类\n",
    "Patients_K_exter=  Patients_K[['CaseID'] +[col for col in Patients_K.columns if col in significant_features]].copy()\n",
    "max_K_values = Patients_K_exter.iloc[:, 1:].max()\n",
    "\n",
    "cluster_labels_validation_all = df_validation_standardized\n",
    "ptid_K0_test = []\n",
    "Patients_K_test = pd.DataFrame()\n",
    "cluster_labels_all_test = pd.DataFrame()\n",
    "ptid = []\n",
    "Patients_covtype_test = []\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "\n",
    "for ptid, group in cluster_labels_validation_all.groupby('CaseID'):\n",
    "    try:\n",
    "        analysis_data = group.drop(['CaseID'], axis=1)\n",
    "        K, cov_type_list = assign_K_value2features(analysis_data.drop(['ID'], axis=1), K_num) \n",
    "        # K_limited = [min(k, max_k) for k, max_k in zip(K, max_K_values)]\n",
    "        K_limited = K\n",
    "\n",
    "        if sum(K_limited) != 0:\n",
    "            cluster_labels_df = cluster_patient_features(analysis_data.drop(['ID'], axis=1), K_limited, cov_type_list)\n",
    "\n",
    "            cluster_labels_df['ID'] = analysis_data[\"ID\"].values \n",
    "            cluster_labels_df['CaseID'] = ptid\n",
    "\n",
    "            cols = cluster_labels_df.columns.tolist()\n",
    "            cols = ['ID', 'CaseID'] + [col for col in cols if col not in ['ID', 'CaseID']]\n",
    "            cluster_labels_df = cluster_labels_df[cols]\n",
    "            cluster_labels_all_test = pd.concat([cluster_labels_all_test, cluster_labels_df], ignore_index=True, axis=0)\n",
    "\n",
    "            K_new = [ptid] + K_limited  \n",
    "            K_new_df=pd.DataFrame([K_new], columns=['CaseID'] +  list(cluster_labels_validation_all.columns.drop(['ID','CaseID'])))\n",
    "            Patients_K_test = pd.concat([Patients_K_test, K_new_df], ignore_index=True, axis=0)\n",
    "           # print(f'{ptid} is done')\n",
    "        else:\n",
    "            ptid_K0_test.append(ptid)\n",
    "            print(f'something wrong: K is 0 for patient {ptid}')\n",
    "    except Exception as e:\n",
    "        print(f'Error: something wrong about K for patient {ptid}, Error: {e}')\n",
    "    finally:\n",
    "        Patients_covtype_test.append((ptid, cov_type_list))\n",
    "\n",
    "warnings.filterwarnings('always', category=UserWarning, module='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Patients_K_test_IDrep = pd.merge(inval_set_pyradiomics_SV[['CaseID', 'ID_rep']].drop_duplicates(), Patients_K_test, on='CaseID', how='left')\n",
    "Patients_K_test_IDrep.to_excel(os.path.join(cluster_dir,\"inval_featurelevel_Kvalue.xlsx\"), index=False)\n",
    "cluster_labels_all_test.to_excel(os.path.join(source_path,\"results\",\"cluster_label\",\"inval_featurelevel_cluster.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "外部测试集的聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##外部测试集\n",
    "testing_pyradiomics_SV = pd.read_excel(os.path.join(cluster_dir,'Feature_level','Testing_featurelevel_pyradiomics.xlsx'))\n",
    "testing_pyradiomics_SV['CaseID'] = testing_pyradiomics_SV['ID'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "\n",
    "columns_to_keep = ['CaseID','ID'] + [col for col in testing_pyradiomics_SV.columns if col in significant_features]  # 生成要保留的列名列表\n",
    "testing_pyradiomics_SV = testing_pyradiomics_SV[columns_to_keep].copy()  # 保留指定的列\n",
    "\n",
    "## 【去掉有错误的行】使用applymap和pd.to_numeric尝试将所有值转换为数值，无法转换的设置为NaN\n",
    "df_numeric = testing_pyradiomics_SV.drop([\"ID\",'CaseID'], axis=1).apply(pd.to_numeric, errors='coerce')\n",
    "rows_with_non_numeric = df_numeric.isnull().any(axis=1)\n",
    "testing_pyradiomics_cluster_test_SV = testing_pyradiomics_SV[~rows_with_non_numeric].reset_index(drop=True)\n",
    "\n",
    "features_to_scale_validation_SV = testing_pyradiomics_cluster_test_SV.drop(['CaseID','ID'], axis=1)\n",
    "scaled_features_validation_SV  = training_scaler.transform(features_to_scale_validation_SV)\n",
    "\n",
    "scaled_features_validation_df = pd.DataFrame(scaled_features_validation_SV, columns=features_to_scale_validation_SV.columns)\n",
    "df_validation_standardized = pd.concat([testing_pyradiomics_cluster_test_SV[['CaseID', 'ID']], scaled_features_validation_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 开始聚类\n",
    "Patients_K_exter=  Patients_K[['CaseID'] +[col for col in Patients_K.columns if col in significant_features]].copy()\n",
    "max_K_values = Patients_K_exter.iloc[:, 1:].max()\n",
    "\n",
    "cluster_labels_validation_all = df_validation_standardized\n",
    "ptid_K0_test = []\n",
    "Patients_K_test = pd.DataFrame()\n",
    "cluster_labels_all_test = pd.DataFrame()\n",
    "ptid = []\n",
    "Patients_covtype_test = []\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "\n",
    "for ptid, group in cluster_labels_validation_all.groupby('CaseID'):\n",
    "    try:\n",
    "        analysis_data = group.drop(['CaseID'], axis=1)\n",
    "        K, cov_type_list = assign_K_value2features(analysis_data.drop(['ID'], axis=1), K_num) \n",
    "        # K_limited = [min(k, max_k) for k, max_k in zip(K, max_K_values)]\n",
    "        K_limited = K\n",
    "\n",
    "        if sum(K_limited) != 0:\n",
    "            cluster_labels_df = cluster_patient_features(analysis_data.drop(['ID'], axis=1), K_limited, cov_type_list)\n",
    "\n",
    "            cluster_labels_df['ID'] = analysis_data[\"ID\"].values  # 假设analysis_data和cluster_labels_df行对齐\n",
    "            cluster_labels_df['CaseID'] = ptid\n",
    "\n",
    "            # 调整列的顺序，将ID和CaseID放在前面\n",
    "            cols = cluster_labels_df.columns.tolist()\n",
    "            cols = ['ID', 'CaseID'] + [col for col in cols if col not in ['ID', 'CaseID']]\n",
    "            cluster_labels_df = cluster_labels_df[cols]\n",
    "            cluster_labels_all_test = pd.concat([cluster_labels_all_test, cluster_labels_df], ignore_index=True, axis=0)\n",
    "\n",
    "            K_new = [ptid] + K_limited   #str 和list组合需要有[xxx]\n",
    "            K_new_df=pd.DataFrame([K_new], columns=['CaseID'] +  list(cluster_labels_validation_all.columns.drop(['ID','CaseID'])))\n",
    "            Patients_K_test = pd.concat([Patients_K_test, K_new_df], ignore_index=True, axis=0)\n",
    "            #print(f'{ptid} is done')\n",
    "        else:\n",
    "            ptid_K0_test.append(ptid)\n",
    "            print(f'something wrong: K is 0 for patient {ptid}')\n",
    "    except Exception as e:\n",
    "        print(f'Error: something wrong about K for patient {ptid}, Error: {e}')\n",
    "    finally:\n",
    "        Patients_covtype_test.append((ptid, cov_type_list))\n",
    "        #大概2mins 28.6s\n",
    "\n",
    "warnings.filterwarnings('always', category=UserWarning, module='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_pyradiomics_SV['CaseID'] = testing_pyradiomics_SV['ID'].apply(lambda x: '_'.join(x.split('_')[:1]))\n",
    "Patients_K_test_IDrep = pd.merge(testing_pyradiomics_SV[['CaseID']].drop_duplicates(), Patients_K_test, on='CaseID', how='left')\n",
    "Patients_K_test_IDrep.to_excel(os.path.join(cluster_dir,\"testing_featurelevel_Kvalue.xlsx\"), index=False)\n",
    "cluster_labels_all_test.to_excel(os.path.join(source_path,\"results\",\"cluster_label\",\"testing_featurelevel_cluster.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TCIA-TCGA数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TCIA-HCC-TACE\n",
    "TCIA_TCGA_pyradiomics_SV = pd.read_excel(os.path.join(cluster_dir,'Feature_level','TCIA_TCGA_featurelevel_pyradiomics.xlsx'))\n",
    "TCIA_TCGA_pyradiomics_SV['CaseID'] = TCIA_TCGA_pyradiomics_SV['ID'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "\n",
    "TCIA_TCGA_pyradiomics_SV = TCIA_TCGA_pyradiomics_SV[columns_to_keep].copy()  \n",
    "\n",
    "df_numeric = TCIA_TCGA_pyradiomics_SV.drop([\"ID\",'CaseID'], axis=1).apply(pd.to_numeric, errors='coerce')\n",
    "rows_with_non_numeric = df_numeric.isnull().any(axis=1)\n",
    "TCIA_TCGA_pyradiomics_cluster_test_SV = TCIA_TCGA_pyradiomics_SV[~rows_with_non_numeric].reset_index(drop=True)\n",
    "\n",
    "features_to_scale_TCIA_TCGA_SV = TCIA_TCGA_pyradiomics_cluster_test_SV.drop(['CaseID','ID'], axis=1)\n",
    "scaled_features_TCIA_TCGA_SV  = training_scaler.transform(features_to_scale_TCIA_TCGA_SV)\n",
    "\n",
    "\n",
    "scaled_features_TCIA_TCGA_df = pd.DataFrame(scaled_features_TCIA_TCGA_SV, columns=features_to_scale_TCIA_TCGA_SV.columns)\n",
    "df_TCIA_TCGA_standardized = pd.concat([TCIA_TCGA_pyradiomics_cluster_test_SV[['CaseID', 'ID']], scaled_features_TCIA_TCGA_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 开始聚类\n",
    "cluster_labels_TCIA_TCGA_all = df_TCIA_TCGA_standardized\n",
    "ptid_K0_TCIA_TCGA = []\n",
    "Patients_K_TCIA_TCGA = pd.DataFrame()\n",
    "cluster_labels_all_TCIA_TCGA = pd.DataFrame()\n",
    "ptid = []\n",
    "Patients_covtype_TCIA_TCGA = []\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "for ptid, group in cluster_labels_TCIA_TCGA_all.groupby('CaseID'):\n",
    "    try:\n",
    "        analysis_data = group.drop(['CaseID'], axis=1)\n",
    "        K, cov_type_list = assign_K_value2features(analysis_data.drop(['ID'], axis=1), K_num) \n",
    "        # K_limited = [min(k, max_k) for k, max_k in zip(K, max_K_values)]\n",
    "        K_limited = K\n",
    "\n",
    "        if sum(K_limited) != 0:\n",
    "            cluster_labels_df = cluster_patient_features(analysis_data.drop(['ID'], axis=1), K_limited, cov_type_list)\n",
    "\n",
    "            cluster_labels_df['ID'] = analysis_data[\"ID\"].values  # 假设analysis_data和cluster_labels_df行对齐\n",
    "            cluster_labels_df['CaseID'] = ptid\n",
    "\n",
    "            # 调整列的顺序，将ID和CaseID放在前面\n",
    "            cols = cluster_labels_df.columns.tolist()\n",
    "            cols = ['ID', 'CaseID'] + [col for col in cols if col not in ['ID', 'CaseID']]\n",
    "            cluster_labels_df = cluster_labels_df[cols]\n",
    "            cluster_labels_all_TCIA_TCGA = pd.concat([cluster_labels_all_TCIA_TCGA, cluster_labels_df], ignore_index=True, axis=0)\n",
    "\n",
    "            K_new = [ptid] + K_limited   #str 和list组合需要有[xxx]\n",
    "            K_new_df=pd.DataFrame([K_new], columns=['CaseID'] +  list(cluster_labels_validation_all.columns.drop(['ID','CaseID'])))\n",
    "            Patients_K_TCIA_TCGA = pd.concat([Patients_K_TCIA_TCGA, K_new_df], ignore_index=True, axis=0)\n",
    "            #print(f'{ptid} is done')\n",
    "        else:\n",
    "            ptid_K0_TCIA_TCGA.append(ptid)\n",
    "            print(f'something wrong: K is 0 for patient {ptid}')\n",
    "    except Exception as e:\n",
    "        print(f'Error: something wrong about K for patient {ptid}, Error: {e}')\n",
    "    finally:\n",
    "        Patients_covtype_test.append((ptid, cov_type_list))\n",
    "        #大概2mins 28.6s\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Patients_K_TCIA_TCGA.to_excel(os.path.join(cluster_dir,\"TCIA_TCGA_featurelevel_Kvalue.xlsx\"), index=False)\n",
    "cluster_labels_all_TCIA_TCGA.to_excel(os.path.join(source_path,\"results\",\"cluster_label\",\"TCIA_TCGA_featurelevel_cluster.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features(df_stand, id_columns):\n",
    "    if isinstance(id_columns, str): \n",
    "        id_columns = [id_columns]\n",
    "    df_stand.reset_index(drop=True, inplace=True)   \n",
    "    features_to_scale = df_stand.drop(columns=id_columns)   \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features_to_scale)\n",
    "    \n",
    "    scaled_features_df = pd.DataFrame(scaled_features, columns=features_to_scale.columns)\n",
    "    \n",
    "    id_columns_df = df_stand[id_columns]\n",
    "    \n",
    "    result_df = pd.concat([id_columns_df, scaled_features_df], axis=1)\n",
    "    \n",
    "    return result_df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pyradiomics = (\"/path/to/training_data\")\n",
    "invalset_pyradiomics = (\"/path/to/invalset_data\")\n",
    "\n",
    "combined_removed_features_patientlevel = pd.read_excel('/path/to/Removed_features_patientlevel.xlsx')\n",
    "prefixes_features_patientlevel_remove = combined_removed_features_patientlevel[\"removed_Feature\"].tolist()\n",
    "\n",
    "train_set = training_pyradiomics.drop(columns=prefixes_features_patientlevel_remove, errors='ignore').reset_index(drop=True)\n",
    "inval_set = invalset_pyradiomics.drop(columns=prefixes_features_patientlevel_remove, errors='ignore').reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
